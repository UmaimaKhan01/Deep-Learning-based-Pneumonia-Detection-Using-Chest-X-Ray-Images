# -*- coding: utf-8 -*-
"""final med a1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ZMyz-Zl6sF7rJchkuwy5CpPlImXaQVM
"""

from google.colab import drive
drive.mount('/content/drive')

# Dataset path
dataset_path = '/content/drive/MyDrive/archive/chest_xray/chest_xray'

!pip install torch torchvision tensorboard

!pip install torchcam

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms, models
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import classification_report
from torchcam.methods import GradCAM
from torchcam.utils import overlay_mask
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

train_transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Smaller size for limited resources
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_test_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load datasets
train_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'train'), transform=train_transform)
val_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'val'), transform=val_test_transform)
test_dataset = datasets.ImageFolder(os.path.join(dataset_path, 'test'), transform=val_test_transform)

# Use a subset of the dataset for faster training
train_subset = Subset(train_dataset, indices=range(0, len(train_dataset), 10))  # Use every 10th sample
val_subset = Subset(val_dataset, indices=range(0, len(val_dataset), 10))
test_subset = Subset(test_dataset, indices=range(0, len(test_dataset), 10))

# Create data loaders
batch_size = 16  # Smaller batch size due to limited GPU memory
train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)

model_scratch = models.resnet18(weights=None)
num_features = model_scratch.fc.in_features
model_scratch.fc = nn.Linear(num_features, 2)  # Binary classification (Pneumonia/Normal)
model_scratch.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

model_finetune = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
num_features = model_finetune.fc.in_features
model_finetune.fc = nn.Linear(num_features, 2)  # Binary classification (Pneumonia/Normal)

# Freeze all layers except the final fully connected layer
for param in model_finetune.parameters():
    param.requires_grad = False
model_finetune.fc.requires_grad_(True)

model_finetune.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

def train_model(model, criterion, optimizer, num_epochs=5):
    writer = SummaryWriter()
    best_val_acc = 0.0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch+1}/{num_epochs}')
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
                dataloader = train_loader
            else:
                model.eval()
                dataloader = val_loader

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in dataloader:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            writer.add_scalar(f'{phase} Loss', epoch_loss, epoch)
            writer.add_scalar(f'{phase} Accuracy', epoch_acc, epoch)

            if phase == 'val' and epoch_acc > best_val_acc:
                best_val_acc = epoch_acc
                torch.save(model.state_dict(), 'best_model.pth')

    writer.close()
    return model

# Define device (GPU if available, otherwise CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
criterion = nn.CrossEntropyLoss()
optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.001)

# Train the model
model_scratch = train_model(model_scratch, criterion, optimizer_scratch, num_epochs=5)

criterion = nn.CrossEntropyLoss()
optimizer_finetune = optim.Adam(model_finetune.fc.parameters(), lr=0.001)

# Train the model
model_finetune = train_model(model_finetune, criterion, optimizer_finetune, num_epochs=5)

def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (preds == labels).sum().item()
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = correct / total
    print(f'Test Accuracy: {accuracy:.4f}')
    print(classification_report(all_labels, all_preds, target_names=['Normal', 'Pneumonia']))
    return accuracy

# Evaluate both models
print("Evaluating model trained from scratch:")
evaluate_model(model_scratch, test_loader)

print("Evaluating fine-tuned model:")
evaluate_model(model_finetune, test_loader)

def visualize_failure_cases(model, test_loader):
    # Temporarily unfreeze the target layer for Grad-CAM
    target_layer = model.layer4[-1]  # Use the last block of layer4
    for param in target_layer.parameters():
        param.requires_grad = True

    # Initialize Grad-CAM
    cam_extractor = GradCAM(model, target_layer=target_layer)

    misclassified_images = []

    # Collect misclassified images
    model.eval()
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        for i in range(len(preds)):
            if preds[i] != labels[i]:
                misclassified_images.append(inputs[i].cpu())

    # Visualize first 5 failure cases
    for img_tensor in misclassified_images[:5]:
        input_tensor = img_tensor.unsqueeze(0).to(device)
        out = model(input_tensor)
        activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)

        # Convert tensors to images
        img = img_tensor.permute(1, 2, 0).numpy()  # Convert CHW -> HWC
        img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]
        img = (img * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8

        # Handle grayscale images (if applicable)
        if img.shape[-1] == 1:  # Single channel (grayscale)
            img = img.squeeze(-1)  # Remove channel dimension
            img_pil = Image.fromarray(img, mode='L')  # Convert to grayscale PIL image
        else:
            img_pil = Image.fromarray(img)  # Convert to RGB PIL image

        # Ensure activation map is 2D
        activation_map_np = activation_map[0].cpu().numpy()
        if activation_map_np.ndim > 2:
            activation_map_np = activation_map_np.squeeze()  # Remove extra dimensions

        # Overlay activation map on the image
        result = overlay_mask(
            img_pil,
            Image.fromarray((activation_map_np * 255).astype('uint8'), mode='L'),  # Activation map as grayscale
            alpha=0.5
        )
        plt.imshow(result)
        plt.axis('off')
        plt.show()

    # Re-freeze the target layer after visualization
    for param in target_layer.parameters():
        param.requires_grad = False

visualize_failure_cases(model_finetune, test_loader)

import matplotlib.pyplot as plt

# Task 1.1: Training from Scratch
train_losses_task1 = [0.4429, 0.1972, 0.1471, 0.1303, 0.1483]
val_losses_task1 = [0.4345, 0.1574, 1.5979, 1.9694, 2.7181]

# Task 1.2: Fine-Tuning Pre-trained Model
train_losses_task2 = [0.4325, 0.3137, 0.2550, 0.2434, 0.2340]
val_losses_task2 = [0.4967, 0.4211, 0.3392, 0.3222, 0.3518]

# Plot loss curves for Task 1.1
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses_task1, label='Training Loss', marker='o')
plt.plot(val_losses_task1, label='Validation Loss', marker='x')
plt.title('Task 1.1: Training from Scratch', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(fontsize=12)
plt.grid(True)

# Plot loss curves for Task 1.2
plt.subplot(1, 2, 2)
plt.plot(train_losses_task2, label='Training Loss', marker='o')
plt.plot(val_losses_task2, label='Validation Loss', marker='x')
plt.title('Task 1.2: Fine-Tuning Pre-trained Model', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(fontsize=12)
plt.grid(True)

plt.tight_layout()
plt.show()

